# 7IADTChallenge1
1st FIAP AI tech challenge

## Code Description
This code was created to use AI models in breast cancer diagnostics.

## Important Notes
__Breast Cancer Data:__ This code was developed using Kaggle breast cancer database.
The source of data used on this project is available at: https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data The file is saved at: '/data/breast_cancer_data_raw.csv'

__Project CSV data (mofified):__ During the development, tests and execution I had some issues with the original format.
I decided to create the code /utils/clean_csv.py to execute two changes in the file:
* Delete the last column of the file that did not have any data (NaN values).
* Replace " " space character by "_" underline in file column names. For example, "concave points_mean" was replaced by "concave_points_mean".
These changes were helpfull to keep a patter in feature names and usefull when testing the model locally or through the API code I created to expose it.

This code will generate the file: '/data/breast_cancer_data.csv'
This is the file used to train, test, create and validate the models.

## Project Structure

### /analysis
Created three codes to analyse breast cancer data.
* eda.py : Exploration Data Analysis.
* explainability.py : Feature importance, SHAP and local explanation.
* modeling.py : Test the models create a table of results and show calibartion curve.

### /api
Expose models through Rest API
* main.py : Using Fast API create endpoints and initiate the server.
* model.py : function used to receive data and predict the diagnostic using models generated by the code.
* schemas.py : contains the request body schema

### /data
Store patients data.
* breast_cancer_data_raw.csv : original file downloaded from Kaggle.
* breast_cancer_data.csv : manipulated file generated when /utils/clean_csv.py is executed.
* patients_test.csv : test data used to test the model.

### /models
Store generated models, scaler and feature names.
This folder is not being published to github it is in __.gitignore__ file.
* scaler.pkl : Store the data scale. The /src/pre_process.py scale the data that will be used by the model. This scale is stored to be used on tests and predictions.
* feature_names.pkl : Store the list of feature names. These names are the column names of the breast cancer diagnostics file.
* logistic_regression.pkl : model created using Logistic Regression.
* random_forest.pkl : model created using Random Forrest.
* svm.pkl : model created using SVM.
* best_model.pkl : best model between the modles used.

### /results
Storing results of analysis, tests and model creation.
Each analysis code have its own folder. You can see the folder the result is being saved in the console.

### /src
In the division of the code the src folder contains only the file __pre_process.py__ that is being used to process the data before expose it to the models.

### /test_model
Store some codes created for testing.
* new_test_model.py : Using patients data from 'data/patients_test.csv' Predict the diagnostic using the best model (Logistic Regression).
* svm_vs_logistic_comparison.py : Using patients data from 'data/patients_test.csv' compare the diagnostic prediction between SVM and Logistic Regretion models.
* test_model.py : Using static data in the code predict diagnostic using the best model (Logistic Regression).

### /utils
Store clean_csv.py code that is being used to prepare the data.

### main.py
Code created to:
- load the data;
- Training and assesment;
- Test the model;
- Store models in /models folder;
- Choose the best model;
- Display the results.

### Code initialization and details
I created a Dockerfile that will implement this code in a container.
When running it with Docker the sequence of codes below will be executed:
- main.py : will be executed to generate all models.
- eda.py | explainability.py | modeling.py : All analysis code will run and generate graphics and reports.
- api.main.py : will be executed with uvicorn to keep the API endpoint running.
The initialization of /api/main.py will expose these models through API in your computer port 8000 (When running the code please ensure this port is not being used by any other application).
You can access API documentation in the URL: __http://localhost:8000/docs__
I did create a Postman collection as well, this collection is available in the folder __/utils/FIAP_Tech_Challenge_1.postman_collection.json__
All graphics and reports are available to download in the link: __http://localhost:8000/templates/results.html__

#### Running the code with Docker
Ensure you have Docker installed in your computer.
Execute steps below:
```
docker build -t 7iadtchallenge1 .
docker run -p 8000:8000 7iadtchallenge1
```
The first line will build your docker image.
The second line will run it.

### Extra Challenge (Computer Vision Diagnostic)
This extra challenge code was developed but it is not being initialized in Docker.
The reason is because of the amount of data downloaded to run it is close to 5GB.
The data source is available at: https://www.kaggle.com/datasets/awsaf49/cbis-ddsm-breast-cancer-image-dataset/data
Follow the steps below to run and test this extra code:

1. Clone this repository: (help link) https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository
2. Download dataset from the url below, extract data and copy csv and jpeg folders in project path /computer_vision_diagnostic/data/raw
3. Create new virtual environment and install requirements.txt:
```
python -m venv my_env
source venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```
4. Run python scripts in the sequence of names using python as below:
- 1_organize_data.py : will organize the CSV and jpeg files data. Changing the format of the CSV files and storing it in the folder /computer_vision_diagnostic/data/updated.
Will move jpeg files into train and test folders.
- 2_train.py : will be training and saving the model and generating a graphic report.
- 3_evaluate.py : will evaluate the test data and display the results.
- 4_predict.py : in this code we are using the model to predict some exams. These exames are not related with any initial one and we are downloading them from internet.