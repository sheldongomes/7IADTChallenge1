# 7IADTChallenge1
1st FIAP AI tech challenge

## Code Description
This code was created to use AI models in breast cancer diagnostics.

## Important Notes
__Breast Cancer Data:__ This code was developed using Kaggle breast cancer database.
The source of data used on this project is available at: https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data The file is saved at: '/data/breast_cancer_data_raw.csv'

__Project CSV data (mofified):__ During the development, tests and execution I had some issues with the original format.
I decided to create the code /utils/clean_csv.py to execute two changes in the file:
* Delete the last column of the file that did not have any data (NaN values).
* Replace " " space character by "_" underline in file column names. For example, "concave points_mean" was replaced by "concave_points_mean".
These changes were helpfull to keep a patter in feature names and usefull when testing the model locally or through the API code I created to expose it.

This code will generate the file: '/data/breast_cancer_data.csv'
This is the file used to train, test, create and validate the models.

## Project Structure

### /analysis
Created three codes to analyse breast cancer data.
* eda.py : Exploration Data Analysis.
* explainability.py : Feature importance, SHAP and local explanation.
* modeling.py : Test the models create a table of results and show calibartion curve.

### /api
Expose models through Rest API
* main.py : Using Fast API create endpoints and initiate the server.
* model.py : function used to receive data and predict the diagnostic using models generated by the code.
* schemas.py : contains the request body schema

### /data
Store patients data.
* breast_cancer_data_raw.csv : original file downloaded from Kaggle.
* breast_cancer_data.csv : manipulated file generated when /utils/clean_csv.py is executed.
* patients_test.csv : test data used to test the model.

### /models
Store generated models, scaler and feature names.
This folder is not being published to github it is in __.gitignore__ file.
* scaler.pkl : Store the data scale. The /src/pre_process.py scale the data that will be used by the model. This scale is stored to be used on tests and predictions.
* feature_names.pkl : Store the list of feature names. These names are the column names of the breast cancer diagnostics file.
* logistic_regression.pkl : model created using Logistic Regression.
* random_forest.pkl : model created using Random Forrest.
* svm.pkl : model created using SVM.
* best_model.pkl : best model between the modles used.

### /results
Storing results of analysis, tests and model creation.
Each analysis code have its own folder. You can see the folder the result is being saved in the console.

### /src
In the division of the code the src folder contains only the file __pre_process.py__ that is being used to process the data before expose it to the models.

### /test_model
Store some codes created for testing.
* new_test_model.py : Using patients data from 'data/patients_test.csv' Predict the diagnostic using the best model (Logistic Regression).
* svm_vs_logistic_comparison.py : Using patients data from 'data/patients_test.csv' compare the diagnostic prediction between SVM and Logistic Regretion models.
* test_model.py : Using static data in the code predict diagnostic using the best model (Logistic Regression).

### /utils
Store clean_csv.py code that is being used to prepare the data.

### main.py
Code created to:
- load the data;
- Training and assesment;
- Test the model;
- Store models in /models folder;
- Choose the best model;
- Display the results.

### Code initialization and details
I created a Dockerfile that will implement this code in a container.
When running it with Docker the main.py code will be executed to generate all models.
A second command will be used to initialize /api/main.py that will expose these models through API in your computer port 8000 (When running the code please ensure this port is not being used by any other application).
You can access API documentation in the URL: http://localhost:8000/docs
I did create a Postman collection as well, this collection is available in the folder /utils/FIAP_Tech_Challenge_1.postman_collection.json

#### Running the code
Ensure you have Docker installed in your computer.
Execute steps below:

docker build -t 7iadtchallenge1 . # this command will build your docker file
docker run -p 8000:8000 7iadtchallenge1 # this command will run your docker file